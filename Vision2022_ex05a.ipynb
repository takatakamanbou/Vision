{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vision2022-ex05a.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/Vision/blob/main/Vision2022_ex05a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDBFFtt8-EUa"
      },
      "source": [
        "# Vision2022-ex05a\n",
        "\n",
        "課題の期限や提出の方法などについては，Visionチーム内に書いてます．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI3mY1FGZWgV"
      },
      "source": [
        "\n",
        "## PyTorch でロジスティック回帰\n",
        "\n",
        "この課題とそれに続く課題では，[PyTorch](https://pytorch.org/) という深層学習フレームワークを用いて，ニューラルネットの学習の実験をやってみます．まずは，ニューラルネットの前に，ロジスティック回帰から\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "seaborn.set()\n",
        "\n",
        "import torch\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "eKF0PBp52ThL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset \n",
        "\n",
        "PyTorch のプログラムでは，データを Dataset クラス（torch.utils.data.Dataset）のオブジェクトとして扱うと便利．\n",
        "ここでは，MNIST データのクラスとしてすでに用意されている [torchvision.datasets.MNIST](https://pytorch.org/vision/stable/datasets.html#mnist) クラスを使ってみる．\n",
        "自前のデータセットを使う場合は，Dataset クラスを拡張して自分のクラスを作ればよい．"
      ],
      "metadata": {
        "id": "2l5d8fsN238g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST クラスのインスタンスを生成．学習データとテストデータ．\n",
        "以下のセルを初めて実行したときは，カレントディレクトリの下の `data/MNIST` 以下に MNIST のデータがダウンロードされる．2回目以降はデータが存在していれば再ダウンロードはしない．\n",
        "\n",
        "`transform` オプションに指定しているものの意味については，今の時点ではごく簡単に．\n",
        "\n",
        "PyTorch では，データを **Tensor** という形式で扱う．NumPy の array と同じような配列と思ってよい．数学の概念である「テンソル」に似てなくもない．Dataset クラスでは，`transform` オプションにいろいろ指定することで，例えば画像だったら拡大縮小したり一部を切り取ったりと様々な前処理を施すことができるようになっている．\n",
        "ここでは，最小限の前処理として，ファイルから読み込んだ画素値を Tensor に変換するよう指示している．"
      ],
      "metadata": {
        "id": "JYhDoPl62-V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習データのインスタンス\n",
        "dsL = MNIST(root='data', train=True, download=True, transform=ToTensor())\n",
        "\n",
        "# テストデータのインスタンス\n",
        "dsT = MNIST(root='data', train=False, download=True, transform=ToTensor())"
      ],
      "metadata": {
        "id": "3mY8C0SB3FuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`dsL` の最初の `N` 個のデータを取り出して眺めてみる．"
      ],
      "metadata": {
        "id": "uLw0zcJO3Nf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 10\n",
        "\n",
        "fig = plt.figure(figsize=(10, 2))\n",
        "for n in range(N):\n",
        "    x, y = dsL[n]  # n 番目の学習データを取り出す\n",
        "    fig.add_subplot(1, N, n+1)\n",
        "    plt.title(f'class{y}')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(x.squeeze(), cmap='gray')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_-VClYgB3MqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "最初の要素を取り出してみる．`x` は 32bit 浮動小数点数を要素にもつ 1x28x28 のテンソル．画素値が 0 から 1 の値で入っている．\n",
        "`y` はクラスラベルを表す整数値．"
      ],
      "metadata": {
        "id": "iAcReZwU3e5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = dsL[0]\n",
        "print(x.shape, x.dtype)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "Z4vnmPLQ3hY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "id": "6gMWVLdx3hXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataloader\n",
        "\n",
        "機械学習では，大量の学習データを扱うことになるので，すべてを一度にメモリに読み込んでしまうのではなく，少しずつ読み込みながら処理したい場面が多い．また，学習データをランダムな順番で扱ったりもしたい，そういうことが簡単にできるように，PyTorh には [torch.utils.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) というクラスが用意されている．\n",
        "\n",
        "DataLoader のインスタンスを生成する際は，Dataset のインスタンスをコンストラクタの引数に指定する．MNIST でも自前のデータでも，Dataset クラスのサブクラスとして定義しておけば，DataLoader で扱える．"
      ],
      "metadata": {
        "id": "eErUHzx-3odt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習データ\n",
        "dlL = DataLoader(dsL, batch_size=10, shuffle=True)\n",
        "\n",
        "# テストデータ\n",
        "dlT = DataLoader(dsT, batch_size=10, shuffle=False)"
      ],
      "metadata": {
        "id": "p-qwmEAs3tju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のセルを何度か実行してみよう．実行するたびに，`dsL` から `batch_size` 個のデータが取り出されて `X` と `Y` に代入される．"
      ],
      "metadata": {
        "id": "dB-FvWhm3w5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = next(iter(dlL)) # dlL から一つのデータバッチを取り出す\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print(Y)"
      ],
      "metadata": {
        "id": "u-Co0h76351K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次は，`dlT` の方で同じことをしてみよう．"
      ],
      "metadata": {
        "id": "xOaDKpCP4Gx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = next(iter(dlT)) # dlT から一つのデータバッチを取り出す\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print(Y)"
      ],
      "metadata": {
        "id": "Ap10ctdP4FsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`dlT` の方は，`shuffle=False` で生成した DataLoader なので，データを毎回同じ順序で取り出すことになる．上記では毎回イテレータを初期化しているので，実行するたびに `dsT` の先頭から 10 個が取り出される → 毎回同じデータが出てくる，ちうことになる．"
      ],
      "metadata": {
        "id": "9VH5ObbG4NSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ロジスティック回帰のネットワークモデルの定義とインスタンスの生成"
      ],
      "metadata": {
        "id": "YdiSZRgb4bLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch で深層学習のプログラムを作る際は，[torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) のサブクラスとしてニューラルネットワークの構造を定義する．\n",
        "ここでは，ロジスティック回帰モデルを定義してみる．ロジスティック回帰は，一層しかない（中間層のない）最も単純な構造のニューラルネットとみなすことができる．\n",
        "\n",
        "- [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
        "- [torch.nn.LogSoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html)"
      ],
      "metadata": {
        "id": "_wKcAJ_R4d0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Logistic(nn.Module):\n",
        "\n",
        "    # いわゆるコンストラクタ\n",
        "    #    D: 入力の大きさ（データの次元数）  K: 出力の大きさ（クラス数）\n",
        "    def __init__(self, D, K):\n",
        "\n",
        "        # スーパークラスのコンストラクタ呼び出し\n",
        "        super(Logistic, self).__init__()\n",
        "\n",
        "        # インスタンス変数\n",
        "        self.D = D\n",
        "        self.K = K\n",
        "\n",
        "        # 全結合層の定義\n",
        "        self.fc = nn.Linear(D, K)\n",
        "\n",
        "        # softmax（のlog）の定義\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    # 出力を計算するインスタンスメソッドの定義．nn.Module の forward メソッドをオーバーライド\n",
        "    def forward(self, X):\n",
        "\n",
        "        X = self.fc(X)\n",
        "        X = self.logsoftmax(X)\n",
        "        return X"
      ],
      "metadata": {
        "id": "D6izBErE4MUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "現在の環境で GPU を使えるか（CUDAの使えるデバイスがあるかどうか）を調べる．PyTorch で書いたプログラムは，CUDA が使える環境ならそのまま GPU で動かすことができる．\n",
        "\n",
        "Colab はデフォルトでは CPU で動くが，上部のメニューで「ランタイム」> 「ランタイムのタイプを変更」から，「ハードウェアアクセラレータ」に「GPU」を指定すれば，GPUで動かすこともできる．ランタイムを再起動することになるので，一からセルを実行し直す必要あり．"
      ],
      "metadata": {
        "id": "hm2uCHLp4n43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'using {device}')"
      ],
      "metadata": {
        "id": "xlXVv_mL4nS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記で定義した `Logistic` クラスのインスタンスを生成．\n",
        "入力がD次元，出力がK次元でバイアスありの Linear 層ひとつだけのネットワーク．\n",
        "ネットワークのパラメータは適当な乱数で初期化される．"
      ],
      "metadata": {
        "id": "tVy2eLEM5VT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D, K = 100, 3\n",
        "model = Logistic(D, K).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "l-mNMlx15X-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ためしに 5xD のデータを乱数で作り，モデルに入力してみる．出力が 5xK になっていることがわかる．"
      ],
      "metadata": {
        "id": "n41NewH05_nO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(5, D).to(device)\n",
        "Y = model(X)\n",
        "print(X.shape, Y.shape)"
      ],
      "metadata": {
        "id": "IjwlPf0I6D6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習（パラメータ最適化）のための準備"
      ],
      "metadata": {
        "id": "-HDajK2V6MvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch では，ネットワークモデルのパラメータについて，損失関数のパラメータでの微分を自動的に計算してくれる．勾配を手計算してそれをコーディングする，という手間が不要．"
      ],
      "metadata": {
        "id": "nndlWhuI6Ret"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のセルを実行すると，次の二つのことが行われる．\n",
        "- 損失関数の定義．[torch.nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)\n",
        "- `model` のパラメータに対して学習定数 0.01 で Stochastic Gradient Descent する最適化器を用意．\n",
        "[torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)"
      ],
      "metadata": {
        "id": "OEK5y3Kn6Z2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 損失関数を用意．Negative Log-Likelihood = 負の対数尤度 = 交差エントロピー\n",
        "loss_func = nn.NLLLoss(reduction='sum')\n",
        "\n",
        "# 最適化器を用意．SGD\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "zsPXu6Cp6cdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SGD だけでなくもっと凝った最適化アルゴリズムもいろいろ使用可能．https://pytorch.org/docs/stable/optim.html"
      ],
      "metadata": {
        "id": "zM8K9mFM6gPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習\n",
        "\n",
        "ここからは，上記で説明していたコードを改めて書いて，MNISTの識別を学習させてみましょう．"
      ],
      "metadata": {
        "id": "1kNhF7lV6mKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習データの用意"
      ],
      "metadata": {
        "id": "JdNgWtdo6oXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D = 28*28 # データの次元数\n",
        "K = 10    # クラス数\n",
        "\n",
        "# データの用意\n",
        "dsL = MNIST(root='data', train=True, download=True, transform=ToTensor())\n",
        "dlL = DataLoader(dsL, batch_size=100, shuffle=True)\n",
        "\n",
        "# 学習モデルの生成\n",
        "model = Logistic(D, K).to(device)\n",
        "print(model)\n",
        "\n",
        "# 損失関数を用意．Negative Log-Likelihood = 負の対数尤度 = 交差エントロピー\n",
        "loss_func = nn.NLLLoss(reduction='sum')\n",
        "\n",
        "# 最適化器を用意．SGD\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "tgNQgumn6iqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次のセルが学習のループ．深層学習の界隈では，個々の学習データを一回ずつ学習するサイクルを「**epoch**（エポック）」という．以下では学習を 10 エポック繰り返している．\n",
        "\n",
        "この例では，学習データ数が 60000 で batchsize が 100 なので，`for i` のループは 60000 / 100 = 600 回繰り返している．つまり 1 エポックの間にパラメータを 600 回更新している．"
      ],
      "metadata": {
        "id": "bVFQSVRk6xW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nepoch = 10\n",
        "\n",
        "for t in range(nepoch):\n",
        "\n",
        "    loss_sum = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for i, (X, lab) in enumerate(dlL):\n",
        "\n",
        "        # device で指定されたデバイスへ転送\n",
        "        X = X.to(device)\n",
        "        lab = lab.to(device)\n",
        "\n",
        "        # X を (batchsize, 1, 28, 28) から (batchsize, 784) へ reshape\n",
        "        X = X.reshape((-1, D)).to(device)\n",
        "\n",
        "        Y = model(X)           # 一つのバッチ X を入力して出力 Y を計算\n",
        "        loss = loss_func(Y, lab) # 正解ラベル Zt に対する loss を計算\n",
        "\n",
        "        optimizer.zero_grad()  # 勾配をリセット\n",
        "        loss.backward()        # 誤差逆伝播でパラメータ更新量を計算\n",
        "        optimizer.step()       # パラメータを更新\n",
        "\n",
        "        n += len(X)\n",
        "        loss_sum += loss.item()  # 損失関数の値\n",
        "\n",
        "\n",
        "    print(f'{t} {loss_sum/n}')\n"
      ],
      "metadata": {
        "id": "UJ4shepl62F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習後の loss と識別率を算出"
      ],
      "metadata": {
        "id": "uyQ-BV7-8Emw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習データに対する loss と識別率．"
      ],
      "metadata": {
        "id": "beF6hVnN8JTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ネットワークを eval mode にする\n",
        "model.eval()\n",
        "\n",
        "loss_sum = 0.0\n",
        "ncorrect = 0\n",
        "n = 0\n",
        "\n",
        "for i, (X, lab) in enumerate(dlL):\n",
        "\n",
        "    # device で指定されたデバイスへ転送\n",
        "    X = X.to(device)\n",
        "    lab = lab.to(device)\n",
        "\n",
        "    # X を (batchsize, 1, 28, 28) から (batchsize, 784) へ reshape\n",
        "    X = X.reshape((-1, D))\n",
        "\n",
        "    Y = model(X)           # 一つのバッチ X を入力して出力 Y を計算\n",
        "    loss = loss_func(Y, lab) # 正解ラベル Zt に対する loss を計算\n",
        "\n",
        "    n += len(X)\n",
        "    loss_sum += loss.item()  # 損失関数の値\n",
        "    ncorrect += (Y.argmax(dim=1) == lab).sum().item()  # 正解数\n",
        "\n",
        "print(f'# L: loss = {loss_sum/n:.4f}  ncorrect/n = {ncorrect}/{n} = {ncorrect/n:.4f}')\n"
      ],
      "metadata": {
        "id": "eoz8nXd38OF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "テストデータに対する loss と識別率"
      ],
      "metadata": {
        "id": "5vvpM2s680uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset & DataLoader\n",
        "dsT = MNIST(root='data', train=False, download=True, transform=ToTensor())\n",
        "dlT = DataLoader(dsT, batch_size=100, shuffle=False)\n",
        "\n",
        "# ネットワークを eval mode にする\n",
        "model.eval()\n",
        "\n",
        "loss_sum = 0.0\n",
        "ncorrect = 0\n",
        "n = 0\n",
        "\n",
        "for i, (X, lab) in enumerate(dlT):\n",
        "\n",
        "    # device で指定されたデバイスへ転送\n",
        "    X = X.to(device)\n",
        "    lab = lab.to(device)\n",
        "\n",
        "    # X を (batchsize, 1, 28, 28) から (batchsize, 784) へ reshape\n",
        "    X = X.reshape((-1, D))\n",
        "\n",
        "    Y = model(X)           # 一つのバッチ X を入力して出力 Y を計算\n",
        "    loss = loss_func(Y, lab) # 正解ラベル Zt に対する loss を計算\n",
        "\n",
        "    n += len(X)\n",
        "    loss_sum += loss.item()  # 損失関数の値\n",
        "    ncorrect += (Y.argmax(dim=1) == lab).sum().item()  # 正解数\n",
        "\n",
        "print(f'# L: loss = {loss_sum/n:.4f}  ncorrect/n = {ncorrect}/{n} = {ncorrect/n:.4f}')"
      ],
      "metadata": {
        "id": "frIpxMer9Ggy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wTDaz5ie9dd_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}